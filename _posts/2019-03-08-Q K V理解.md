---
layout:     post   				    # 使用的布局（不需要改）
title:      Q K V 理解 				# 标题 
subtitle:   Query Key Value的理解 #副标题
date:       2019-03-08 				# 时间
author:     XH 						# 作者
header-img: img/post-bg-2015.jpg 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - NMT
    - 概念理解
    - NLP
    - transformer
    - attention

---


> 正所谓前人栽树，后人乘凉。
>
> 感谢[ seeInfinite：关于attention机制在nlp中的应用总结](http://blog.csdn.net/qq_41058526/article/details/80578932)
>
> [我的的博客](https://xinghanzzy.github.io/)



# Query Key Value

# 概念理解

### 单词本意

query：n. 疑问，质问；疑问号 ；[计] 查询 vt. 询问；对……表示疑问

key：vt. 键入；锁上；调节…的音调；提供线索

value：n. 值；价值；价格；重要性；确切涵义

###计算机释义

Attention函数的本质可以被描述为一个查询（query）到一系列（键key-值value）对的映射

![attention中Q K V](https://ws1.sinaimg.cn/large/4ac7f217ly1g0vhrmxn0xj20p50azq43.jpg)

在计算attention时主要分为三步:

1. 第一步是将query和每个key进行相似度计算得到权重，常用的相似度函数有点积，拼接，感知机等；
2. 第二步一般是使用一个softmax函数对这些权重进行归一化；
3. 最后将权重和相应的键值value进行加权求和得到最后的attention。

- 目前在NLP研究中，key和value常常都是同一个，即key=value。

### 自己理解

**encoder-decoder的attention计算的Q、K、V就是** 

​	Q:encoder信息

​	K、V：decoder信息

**self-attention中：**

Q = K = V

- 先乘再拆	1：

   函数输入为上一层的输出a，a与一个权重矩阵相乘，拆分为三份既是Q K V

- 卷积做法：

   ```
   Input Embedding: Batch * Length * Hidden
   
   Posional Encoding : （Batch *）Length * Hidden
   
   Encoder Input = Input Embedding + Posional Encoding
   
   Query,Key,Value = Conv(Encoder Input,Hidden,3*Hidden)
   
   ```

   

- 过去做法：

  函数输入为上一层的输出a，三个一样的，dense relu ，然后split