---
layout:     post
title:      深度学习概念：梯度弥散 梯度爆炸 过拟合 batchsize
subtitle:   梯度弥散 梯度爆炸 过拟合 batchsize Adam Attention
date:       2019-03-18
author:     XH
header-img: img/post-bg-debug.png
catalog: true
tags:
    - 深度学习
    - NMT
---


>[解决梯度消失和梯度弥散的方法](https://blog.csdn.net/zhangbaoanhadoop/article/details/82290129 ) 
>
>

# 梯度弥散

# 过拟合

## 定义	

- 所谓过拟合（Overfit），是这样一种现象：一个假设在训练数据上能够获得比其他假设更好的拟合，但是在训练数据外的数据集 上却不能很好的拟合数据。此时我们就叫这个假设出现了overfit的现象。 
- 当一个模型过为复杂之后，它可以很好的记忆每一个训练数据中的随机噪声，却忘记要去学习训练数据中的通用趋势

## 原因

 - 过拟合其中一个可能的成因就是模型的vc维过高，使用了过强的模型复杂度(model complexity)的能力。（参数多并且过训练）  　　
 - 还有一个原因是数据中的噪声，造成了如果完全拟合的话，也许与真实情景的偏差更大。  　　
 - 最后还有一个原因是数据量有限，这使得模型无法真正了解整个数据的真实分布。  　　
	- 权值学习迭代次数足够多(Overtraining),拟合了训练数据中的噪声和训练样例中没有代表性的特征。 	

## 解决

1. 权值衰减  　　在每次迭代过程中以某个小因子降低每个权值,这等效于修改E的定义,加入一个与网络权值的总量相应的惩罚项,此方法的动机是保持权值较小,避免weight decay,从而使学习过程向着复杂决策面的反方向偏。**（L2正则化）**  

2. 适当的stopping criterion（验证集） 

3. **交叉验证方法**

   在可获得额外的数据提供验证集合时工作得很好,但是小训练集合的过度拟合问题更为严重 

   **k-fold交叉方法**:  　　把训练样例分成k份,然后进行k次交叉验证过程,每次使用不同的一份作为验证集合,其余k-1份合并作为训练集合.每个样例会在一次实验中被用作验证样例,在k-1次实验中被用作训练样例;每次实验中,使用上面讨论的交叉验证过程来决定在验证集合上取得最佳性能的迭代次数n*,然后计算这些迭代次数的均值,作为最终需要的迭代次数。 

4. 正则化 

   ​	正则化方法是在损失函数时候改变
   $$
   J(\theta) = J(\theta) + \lambda R(w)
   
   $$

   $$
   R(w) = ||w||_1=\sum_{i}{|w_i|}
   $$

   $$
   R(w) = ||w||_2^2=\sum_{i}{|w_i^2|}   \text {L2正则}
   $$

   ​	通过限制权重的大小，使得模型不能任意拟合训练数据中的随机噪声。

   5. **Dropout正则化**

      ```
      部分置为：0
      其他缩放：1 / (1 - rate)
      ```

   6. 数据！

   > （1）在神经网络模型中，可使用权值衰减的方法，即每次迭代过程中以某个小因子降低每个权值。
   > （2）选取合适的停止训练标准，使对机器的训练在合适的程度；
   > （3）保留验证数据集，对训练成果进行验证；
   > （4）获取额外数据进行交叉验证；
   > （5）正则化，即在进行目标函数或代价函数优化时，在目标函数或代价函数后面加上一个正则项，一般有L1正则与L2正则等


# Batch size

##**Batch_Size三种情况** 

　　Batch_Size（批尺寸）是机器学习中一个重要参数。 

1. 如果数据集比较小，完全可以采用全数据集 （ Full Batch Learning ）的形式，这样做至少有 2 个好处：其一，由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。其二，由于不同权重的梯度值差别巨大，因此选取一个全局的学习率很困难。 

   > - 凸函数收敛于全局极值点，非凸函数可能会收敛于局部极值点 
   > - 每次学习时间过长 
   > - 训练集很大以至于需要消耗大量的内存 
   > - 全量梯度下降不能进行在线模型参数更新 

2. Batch_Size = 1。这就是在线学习（Online Learning）。使用在线学习，每次修正方向以各自样本的梯度方向修正，横冲直撞[各自为政](https://www.baidu.com/s?wd=%E5%90%84%E8%87%AA%E4%B8%BA%E6%94%BF&tn=24004469_oem_dg&rsv_dl=gh_pl_sl_csd)，难以达到收敛。 

   > - 最大的缺点在于每次更新可能并不会按照正确的方向进行，因此可以带来优化波动(扰动) 

3. 如果网络中采用minibatch SGD算法来优化，所以是一个batch一个batch地将数据输入CNN模型中，然后计算这个batch的所有样本的平均损失，即代价函数是所有样本的平均。而batch_size就是一个batch的所包含的样本数，显然batch_size将影响到模型的优化程度和速度。**mini batch只是为了充分利用GPU memory而做出的妥协** 

   > - 选择一个合理的学习速率很难。如果学习速率过小，则会导致收敛速度很慢。如果学习速率过大，那么其会阻碍收敛，即在极值点附近会振荡。
   >
   > - 学习速率调整(又称学习速率调度，Learning rate schedules)[11]试图在每次更新过程中，改变学习速率，如退火。一般使用某种事先设定的策略或者在每次迭代中衰减一个较小的阈值。无论哪种调整方法，都需要事先进行固定设置，这边便无法自适应每次学习的数据集特点[10]。
   >
   > - 模型所有的参数每次更新都是使用相同的学习速率。如果数据特征是稀疏的或者每个特征有着不同的取值统计特征与空间，那么便不能在每次更新中每个参数使用相同的学习速率，那些很少出现的特征应该使用一个相对较大的学习速率。
   >
   > - 对于非凸目标函数，容易陷入那些次优的局部极值点中，如在神经网路中。那么如何避免呢。

## **改变Batch_Size的影响** 

　　在合理范围内，增大 Batch_Size 的好处：内存利用率提高了，大矩阵乘法的并行化效率提高。跑完一次 epoch（全数据集）所需的迭代次数减少，对于相同数据量的处理速度进一步加快。**在一定范围内，一般来说 Batch_Size 越大，其确定的下降方向越准，引起训练震荡越小**。 
　　

> 下面是参考文献中博主给出的实验结果： 
>
> Batch_Size 太小，算法在 200 epoches 内不收敛。 
>
> 随着 Batch_Size 增大，处理相同数据量的速度越快。 
>
> 随着 Batch_Size 增大，达到相同精度所需要的 epoch 数量越来越多。
>
> 由于上述两种因素的矛盾， Batch_Size 增大到某个时候，达到时间上的最优。 
>
> 由于最终收敛精度会陷入不同的局部极值，因此 Batch_Size 增大到某些时候，达到最终收敛精度上的最优。

## **Minibatch**

　　首先说一下采用mini-batch时的权重更新规则。比如mini-batch size设为100，则权重更新的规则为： 
　　　　　　　　　　　![这里写图片描述](https://img-blog.csdn.net/20160424101904528) 
　　也就是将100个样本的梯度求均值，替代online learning方法中单个样本的梯度值： 
　　　　　　　　　　　　　![这里写图片描述](https://img-blog.csdn.net/20160424101915111) 
　　当采用mini-batch时，我们可以将一个batch里的所有样本放在一个矩阵里，利用线性代数库来加速梯度的计算，这是工程实现中的一个优化方法。 
　　一个大的batch，可以充分利用矩阵、线性代数库来进行计算的加速，batch越小，则加速效果可能越不明显。当然batch也不是越大越好，太大了，权重的更新就会不那么频繁，导致优化过程太漫长。

# Adam

> 梯度下降（Gradient Descent）就好比一个人想从高山上奔跑到山谷最低点，用最快的方式（steepest）奔向最低的位置（minimum） 

Adam 算法和传统的随机梯度下降不同。随机梯度下降保持单一的学习率（即 alpha）更新所有的权重，学习率在训练过程中并不会改变。而 Adam 通过计算梯度的一阶矩估计和二阶矩估计而为不同的参数设计独立的自适应性学习率 



> warmup 更新学习率
>
> Adam 快！

# attention 点乘 加法

$$
P(y_i|y_1,...y_{i-1},X) = g(y_{i-1},s_i,c_i)
$$

