---
layout:     post
title:      深度学习概念：梯度弥散 梯度爆炸 过拟合 batchsize
subtitle:   梯度弥散 梯度爆炸 过拟合 batchsize Adam Attention
date:       2019-03-18
author:     XH
header-img: img/post-bg-kuaidi.jpg
catalog: true
tags:
    - 深度学习
    - NMT
---


>自己看的记录笔记




>参考文献
>
>[解决梯度消失和梯度弥散的方法](https://blog.csdn.net/zhangbaoanhadoop/article/details/82290129 ) 
>
>[注意力机制（Attention Mechanism）在自然语言处理中的应用](https://www.cnblogs.com/robert-dlut/p/5952032.html/)
>
>[真正的完全图解Seq2Seq Attention模型](https://zhuanlan.zhihu.com/p/40920384 )
>
>[优化方法总结：SGD，Momentum，AdaGrad，RMSProp，Adam](https://blog.csdn.net/u010089444/article/details/76725843 )
>
>[梯度下降优化算法总结](https://blog.csdn.net/shuzfan/article/details/75675568 )

# 深度学习概念

##梯度弥散 梯度爆炸

​	梯度弥散。使用反向传播算法传播梯度的时候，随着传播深度的增加，梯度的幅度会急剧减小，会导致浅层神经元的权重更新非常缓慢，不能有效学习。这样一来，深层模型也就变成了前几层相对固定，只能改变最后几层的浅层模型。  

​	梯度弥散的问题很大程度上是来源于激活函数的“饱和”。 

**梯度消失：**

> 1. Relu代替sigmoid
> 2. 梯度裁剪
> 3. RNN-LSTM
> 4. 一种新的方法batch normalization

**梯度爆炸：**

>  使用Gradient Clipping(梯度裁剪)。通过Gradient Clipping，将梯度约束在一个范围内，这样不会使得梯度过大。 





##过拟合

###定义

- 所谓过拟合（Overfit），是这样一种现象：一个假设在训练数据上能够获得比其他假设更好的拟合，但是在训练数据外的数据集 上却不能很好的拟合数据。此时我们就叫这个假设出现了overfit的现象。 
- 当一个模型过为复杂之后，它可以很好的记忆每一个训练数据中的随机噪声，却忘记要去学习训练数据中的通用趋势

###原因

 - 过拟合其中一个可能的成因就是模型的vc维过高，使用了过强的模型复杂度(model complexity)的能力。（参数多并且过训练）  　　
 - 还有一个原因是数据中的噪声，造成了如果完全拟合的话，也许与真实情景的偏差更大。  　　
 - 最后还有一个原因是数据量有限，这使得模型无法真正了解整个数据的真实分布。  　　
	- 权值学习迭代次数足够多(Overtraining),拟合了训练数据中的噪声和训练样例中没有代表性的特征。 	

###解决

1. 权值衰减  　　在每次迭代过程中以某个小因子降低每个权值,这等效于修改E的定义,加入一个与网络权值的总量相应的惩罚项,此方法的动机是保持权值较小,避免weight decay,从而使学习过程向着复杂决策面的反方向偏。**（L2正则化）**  

2. 适当的stopping criterion（验证集） 

3. **交叉验证方法**

   在可获得额外的数据提供验证集合时工作得很好,但是小训练集合的过度拟合问题更为严重 

   **k-fold交叉方法**:  　　把训练样例分成k份,然后进行k次交叉验证过程,每次使用不同的一份作为验证集合,其余k-1份合并作为训练集合.每个样例会在一次实验中被用作验证样例,在k-1次实验中被用作训练样例;每次实验中,使用上面讨论的交叉验证过程来决定在验证集合上取得最佳性能的迭代次数n*,然后计算这些迭代次数的均值,作为最终需要的迭代次数。 

4. 正则化 

   ​	正则化方法是在损失函数时候改变
   $$
   J(\theta) = J(\theta) + \lambda R(w)
   $$

   $$
   R(w) = ||w||_1=\sum_{i}{|w_i|}
   $$

   $$
   R(w) = ||w||_2^2=\sum_{i}{|w_i^2|}   \text {L2正则}
   $$

   ​	通过限制权重的大小，使得模型不能任意拟合训练数据中的随机噪声。

   5. **Dropout正则化**

      ```
      部分置为：0
      其他缩放：1 / (1 - rate)
      ```

   6. 数据！

   > （1）在神经网络模型中，可使用权值衰减的方法，即每次迭代过程中以某个小因子降低每个权值。
   >
   > （2）选取合适的停止训练标准，使对机器的训练在合适的程度；
   >
   > （3）保留验证数据集，对训练成果进行验证；
   >
   > （4）获取额外数据进行交叉验证；
   >
   > （5）正则化，即在进行目标函数或代价函数优化时，在目标函数或代价函数后面加上一个正则项，一般有L1正则与L2正则等

##Batch size&SGD

###Batch_Size三种情况（SGD）

　　Batch_Size（批尺寸）是机器学习中一个重要参数。 

1. **Batch Gradient Descent**

   如果数据集比较小，完全可以采用全数据集 （ Full Batch Learning ）的形式，这样做至少有 2 个好处：其一，由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。其二，由于不同权重的梯度值差别巨大，因此选取一个全局的学习率很困难。 

   > - 凸函数收敛于全局极值点，非凸函数可能会收敛于局部极值点 

   > - 每次学习时间过长 
   > - 训练集很大以至于需要消耗大量的内存 
   > - 全量梯度下降不能进行在线模型参数更新 

2. **Stochastic Gradient Descent**

   Batch_Size = 1。这就是在线学习（Online Learning）。使用在线学习，每次修正方向以各自样本的梯度方向修正，横冲直撞[各自为政](https://www.baidu.com/s?wd=%E5%90%84%E8%87%AA%E4%B8%BA%E6%94%BF&tn=24004469_oem_dg&rsv_dl=gh_pl_sl_csd)，难以达到收敛。 

   > - 算法收敛速度快 
   > - 可以在线更新

   > - 最大的缺点在于每次更新可能并不会按照正确的方向进行，因此可以带来优化波动(扰动) 
   > - 容易收敛到局部最优，并且容易被困在鞍点

3. **Mini-batch Gradient Descent**

   如果网络中采用minibatch SGD算法来优化，所以是一个batch一个batch地将数据输入CNN模型中，然后计算这个batch的所有样本的平均损失，即代价函数是所有样本的平均。而batch_size就是一个batch的所包含的样本数，显然batch_size将影响到模型的优化程度和速度。**mini batch只是为了充分利用GPU memory而做出的妥协** 

   > - 选择一个合理的学习速率很难。如果学习速率过小，则会导致收敛速度很慢。如果学习速率过大，那么其会阻碍收敛，即在极值点附近会振荡。
   >
   > - 学习速率调整(又称学习速率调度，Learning rate schedules)[11]试图在每次更新过程中，改变学习速率，如退火。一般使用某种事先设定的策略或者在每次迭代中衰减一个较小的阈值。无论哪种调整方法，都需要事先进行固定设置，这边便无法自适应每次学习的数据集特点[10]。
   >
   > - 模型所有的参数每次更新都是使用相同的学习速率。如果数据特征是稀疏的或者每个特征有着不同的取值统计特征与空间，那么便不能在每次更新中每个参数使用相同的学习速率，那些很少出现的特征应该使用一个相对较大的学习速率。
   >
   > - 对于非凸目标函数，容易陷入那些次优的局部极值点中，如在神经网路中。那么如何避免呢。

###改变Batch_Size的影响

　　在合理范围内，增大 Batch_Size 的好处：内存利用率提高了，大矩阵乘法的并行化效率提高。跑完一次 epoch（全数据集）所需的迭代次数减少，对于相同数据量的处理速度进一步加快。**在一定范围内，一般来说 Batch_Size 越大，其确定的下降方向越准，引起训练震荡越小**。 
　　

> 下面是参考文献中博主给出的实验结果： 
>
> Batch_Size 太小，算法在 200 epoches 内不收敛。 
>
> 随着 Batch_Size 增大，处理相同数据量的速度越快。 
>
> 随着 Batch_Size 增大，达到相同精度所需要的 epoch 数量越来越多。
>
> 由于上述两种因素的矛盾， Batch_Size 增大到某个时候，达到时间上的最优。 
>
> 由于最终收敛精度会陷入不同的局部极值，因此 Batch_Size 增大到某些时候，达到最终收敛精度上的最优。

　　当采用mini-batch时，我们可以将一个batch里的所有样本放在一个矩阵里，利用线性代数库来加速梯度的计算，这是工程实现中的一个优化方法。 
　　一个大的batch，可以充分利用矩阵、线性代数库来进行计算的加速，batch越小，则加速效果可能越不明显。当然batch也不是越大越好，太大了，权重的更新就会不那么频繁，导致优化过程太漫长。

## Momentum

SGD方法的一个缺点是，其**更新方向**完全依赖于当前的batch，因而其更新十分不稳定。

momentum，动量，它[模拟](https://www.baidu.com/s?wd=%E6%A8%A1%E6%8B%9F&tn=24004469_oem_dg&rsv_dl=gh_pl_sl_csd)的是物体运动时的惯性，即更新的时候在一定程度上保留之前更新的方向，同时利用当前batch的梯度微调最终的更新方向。这样一来，可以在一定程度上增加稳定性，从而学习地更快，并且还有一定摆脱局部最优的能力。

利用惯性，即当前梯度与上次梯度进行加权，如果方向一致，则累加导致更新步长变大；如果方向不同，则相互抵消中和导致更新趋向平衡。

动量参数常被设定为0.9或者一个相近的值。

> Nesterov Momentum
>
> 在小球向下滚动的过程中，我们希望小球能够提前知道在哪些地方坡面会上升，这样在遇到上升坡面之前，小球就开始减速。这方法就是Nesterov Momentum，其在**凸优化**中有较强的理论保证收敛。 

## Adagrad

Adagrad算法能够在训练中自动的对learning rate进行调整，对于出现频率较低参数采用较大的α更新

适合处理**稀疏数据** 

对角矩阵 每个对角线位置是1->t轮梯度平方和

缺点是在训练的中后期，分母上梯度平方的累加将会越来越大，从而梯度趋近于0，使得训练提前结束 

## Adadelta

Adadelta： **仅采用一个窗口范围内的梯度平方之和** 

**Adadelta已经无需设置初始学习率了，其可以自动计算并更新学习率** 

## RMSprop

Adagrad会累加之前所有的梯度平方，而RMSprop仅仅是计算对应的平均值，因此可缓解Adagrad算法学习率下降较快的问题 



##Adam

> 梯度下降（Gradient Descent）就好比一个人想从高山上奔跑到山谷最低点，用最快的方式（steepest）奔向最低的位置（minimum） 

> ​     1.Adam算法可以看做是修正后的[Momentum](http://blog.csdn.net/bvl10101111/article/details/72615621)+[RMSProp](http://blog.csdn.net/BVL10101111/article/details/72616378)算法
>
> ​     2.动量直接并入梯度一阶矩估计中（指数加权）
>
> ​     3.Adam通常被认为对超参数的选择相当鲁棒
>
> ​     4.学习率建议为0.001

Adam 算法和传统的随机梯度下降不同。随机梯度下降保持单一的学习率（即 alpha）更新所有的权重，学习率在训练过程中并不会改变。而 Adam 通过计算梯度的一阶矩估计和二阶矩估计而为不同的参数设计独立的自适应性学习率 

Adam(Adaptive Moment Estimation)是另一种自适应学习率的方法。它利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。Adam的优点主要在于经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。 

**Adam = Momentum+RMSProp**  一阶 + 二阶

度的一阶矩估计（First Moment Estimation，即梯度的均值）和二阶矩估计（Second Moment Estimation，即梯度的未中心化的方差）进行综合考虑

> 1. 实现简单，计算高效，对内存需求少
>
> 2. 参数的更新不受梯度的伸缩变换影响
>
> 3. 超参数具有很好的解释性，且通常无需调整或仅需很少的微调
>
> 4. 更新的步长能够被限制在大致的范围内（初始学习率）
>
> 5. 能自然地实现步长退火过程（自动调整学习率）
>
> 6. 很适合应用于大规模的数据及参数的场景
>
> 7. 适用于不稳定目标函数
>
> 8. 适用于梯度稀疏或梯度存在很大噪声的问题
>
> 综合Adam在很多情况下算作默认工作性能比较优秀的优化器。

> warmup 更新学习率
>
> Adam 快！

##attention 乘法 加法

> 所谓乘法 加法 做的是decoder的隐藏状态S_{t-1}和encoder的各个隐层状态h_j
>
> 这里


$$
P(y_i|y_1,...y_{i-1},X) = g(y_{i-1},s_i,c_i)
$$
![img](https://images2015.cnblogs.com/blog/670089/201610/670089-20161012111504671-910168246.png) ![img](https://images2015.cnblogs.com/blog/670089/201610/670089-20161012111506078-902266845.png) 

##GRU

使用sigmod控制